{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have that - \n",
    "\n",
    "z = $w_1x_1+w_2x_2+....+w_mx_m = \\sum_{j=0}^m x_jw_j = w^Tx$\n",
    "\n",
    "Where z is calculated for a given sample. \n",
    "\n",
    "Now let us define a activation function such that -  \n",
    "\n",
    "$$\\phi(z) = (1 \\ if \\ z \\geq \\theta,\\ otherwise -1) $$\n",
    "\n",
    "For simplicity, we can rewrite above equation by bring $\\theta$ to left hand side and define a weight-zero as -$\\theta$ and $x_0$ as 1. We get - \n",
    "\n",
    "$$z = w_0x_0+w_1x_1+w_2x_2+....+w_mx_m = \\sum_{j=0}^m x_jw_j = w^Tx$$ \n",
    "\n",
    "and \n",
    "\n",
    "$$\\phi(z) = (1 \\ if \\ z \\geq 0,\\ otherwise -1) $$\n",
    "\n",
    "Here, **$\\phi(z)$** is our activation function. \n",
    "\n",
    "The whole idea behind the MCP neuron and Rosenblatt's thresholded perceptron model is to use a reductionist approach to mimic how a single neuron in the brain works: it either fires or it doesn't. Thus, Rosenblatt's initial perceptron rule is fairly\n",
    "simple and can be summarized by the following steps:\n",
    "1. Initialize the weights to 0 or small random numbers.\n",
    "2. For each training sample $x(i)$ perform the following steps:\n",
    "  1. Compute the output value $y^ˆ$ .\n",
    "  2. Update the weights.\n",
    "Here, the output value is the class label.\n",
    "\n",
    "See this illustration of perceptron (shown below) where $\\phi(z) = z$. \n",
    "\n",
    "![](images/perceptron.png)\n",
    "\n",
    "It is important to note that the convergence of the perceptron is only guaranteed if the two classes are linearly separable and the learning rate is sufficiently small. If the two classes can't be separated by a linear decision boundary, we can set a maximum number of passes over the training dataset (epochs) and/or a threshold for the number of tolerated misclassifications—the perceptron would never stop updating the weights otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron vs Sigmoid Neuron\n",
    "\n",
    "In perceptron, output values can either be 0 or 1. However, in case of sigmoid neuron, input/output can take any value in the range [0,1]. \n",
    "\n",
    "Also, perceptron curve is essentially a step function. On the other hand, sigmoid function follows S-shaped function with some midway threshold value (usually .5). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = make_moons(n_samples=100, noise=0.25, random_state=3)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "nn = MLPClassifier(solver='lbfgs')\n",
    "nn.fit(X_train, y_train)\n",
    "y_pred = nn.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `activation` keyword can take one of following values - (relu, tanh, identity, logistic). This is the activation function for hidden layers. \n",
    "\n",
    "  - `identity`, no-op activation, useful to implement linear bottleneck, returns $f(x) = x$\n",
    "  - `logistic`, the logistic sigmoid function, returns $f(x) = \\frac{1}{1 + exp(-x)}$.\n",
    "  - `tanh`, the hyperbolic tan function, returns $f(x) = tanh(x)$.\n",
    "  - `relu`, the rectified linear unit function, returns $f(x) = max(0, x)$\n",
    "\n",
    "Also, `solver` keyword can take one of these values - (`lbfgs`, `sgd`, `adam`). Default is `adam`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> from sklearn.datasets import fetch_mldata\n",
    ">>> mnist = fetch_mldata('MNIST original')\n",
    "\n",
    "#second command raised OSError few times. It was because internet speed was erratic and downloaded data got corrupted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((70000, 784),\n",
       " dict_keys(['DESCR', 'COL_NAMES', 'target', 'data']),\n",
       " ['label', 'data'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.data.shape, mnist.keys(), mnist.COL_NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(mnist.data, mnist.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
